# Python Readability Testing Guide

## Overview

This document provides a guide to the testing infrastructure for the Python Readability project. It explains how to run tests, add new tests, and understand the testing approach used in the project.

## Test Structure

The testing infrastructure consists of the following components:

- **Test Cases**: Located in `tests/test-pages/`
- **Test Functions**: Implemented in `tests/test_readability.py`
- **Test Helpers**: Implemented in `tests/conftest.py`
- **Debug Tools**: Implemented in `tests/debug_tools.py`
- **Assessment Documents**: Various Markdown files documenting the testing infrastructure

## How to Run Tests

### Running All Tests

To run all tests, use the following command:

```bash
pytest tests/test_readability.py -v
```

### Running Specific Test Cases

To run a specific test case, use the `-k` option:

```bash
pytest tests/test_readability.py -v -k "test_specific_cases[case_name]"
```

Replace `case_name` with the name of the test case you want to run, for example:

```bash
pytest tests/test_readability.py -v -k "test_specific_cases[001]"
```

### Running the Comprehensive Test

To run the comprehensive test that tests all discovered test cases:

```bash
pytest tests/test_readability.py -v -k "test_all_cases"
```

## Understanding Test Results

When a test fails, the following information is available:

- **HTML Comparison**: The test compares the generated HTML with the expected HTML and calculates a similarity score.
- **Metadata Comparison**: The test compares the extracted metadata with the expected metadata.
- **Debug Output**: The test saves debug output to `tests/debug/{case_name}/`.

### Debug Output Files

- `generated.html`: The HTML generated by the parser
- `expected.html`: The expected HTML
- `diff.html`: An HTML diff showing the differences
- `metadata.json`: The metadata extracted by the parser
- `expected-metadata.json`: The expected metadata

## Adding New Test Cases

To add a new test case, follow these steps:

1. Create a new directory in `tests/test-pages/` with a descriptive name
2. Add the following files to the directory:
   - `source.html`: The input HTML
   - `expected.html`: The expected output HTML
   - `expected-metadata.json`: The expected metadata (optional)
3. Add the test case name to the parameterized test list in `tests/test_readability.py`

You can use the `test-case-migration-script.py` to automate this process:

```bash
python test-case-migration-script.py --all  # Migrate all missing test cases
python test-case-migration-script.py case1 case2  # Migrate specific test cases
```

## Customizing Tests

### HTML Comparison Options

The HTML comparison can be customized with the following options:

- `threshold`: The similarity threshold (default: 0.9)
- `compare_structure`: Whether to compare HTML structure (default: True)
- `normalize_options`: Options for HTML normalization:
  - `normalize_case`: Whether to normalize case (default: True)
  - `normalize_attrs`: Whether to normalize attribute ordering (default: True)
  - `normalize_whitespace`: Whether to normalize whitespace (default: True)
  - `normalize_entities`: Whether to normalize HTML entities (default: True)

### Metadata Comparison Options

The metadata comparison can be customized with the following options:

- `strict`: Whether to be strict in comparison (default: False)
- `field_mapping`: Custom field mapping between JSON and Article attributes

## Advanced Testing

### Test Case Categories

The test cases are organized into the following categories:

1. **Basic Test Cases**: Simple test cases for basic functionality
   - Examples: `001`, `002`, `003-metadata-preferred`, `004-metadata-space-separated-properties`

2. **Core Functionality Tests**: Tests for core functionality
   - Examples: `basic-tags-cleaning`, `normalize-spaces`, `replace-brs`, `metadata-content-missing`

3. **Edge Case Tests**: Tests for edge cases and special scenarios
   - Examples: `hidden-nodes`, `missing-paragraphs`, `svg-parsing`, `comment-inside-script-parsing`

4. **Feature-specific Tests**: Tests for specific features
   - Examples: `base-url`, `remove-script-tags`, `keep-images`

5. **Real-world Website Tests**: Tests with real-world websites
   - Examples: `nytimes-1`, `mozilla-1`, `medium-1`

### Enhanced Debugging

For enhanced debugging, you can use the following techniques:

1. **Clear Debug Directory**: Clear the debug directory to remove old debug output
   ```python
   from tests.debug_tools import clear_debug_directory
   clear_debug_directory()
   ```

2. **Generate HTML Diff**: Generate an HTML diff between two HTML strings
   ```python
   from tests.debug_tools import generate_html_diff
   generate_html_diff(actual_html, expected_html, output_path)
   ```

3. **Save Debug Output**: Save debug output for a test case
   ```python
   from tests.debug_tools import save_debug_output
   save_debug_output(test_name, article, expected_html, expected_metadata)
   ```

## Testing Resources

The following resources are available for understanding and improving the testing infrastructure:

- [Test Verification Plan](./test-verification-plan.md): A systematic approach to verify test implementation
- [Test Case Comparison](./test-case-comparison.md): A comparison of Go and Python test cases
- [Test Function Analysis](./test-function-analysis.md): Analysis of test functions
- [Comparison Function Analysis](./comparison-function-analysis.md): Analysis of comparison functions
- [Debug Tools Analysis](./debug-tools-analysis.md): Analysis of debug tools
- [Test Improvement Plan](./test-improvement-plan.md): A prioritized improvement plan
- [Test Infrastructure Assessment](./test-infrastructure-assessment.md): A comprehensive assessment of the testing infrastructure

## Common Test Issues and Solutions

### Issue: Low Similarity Score

If the HTML similarity score is too low, consider:

1. Checking for formatting differences
2. Normalizing HTML before comparison
3. Adjusting the similarity threshold

### Issue: Metadata Differences

If there are metadata differences, consider:

1. Checking if the difference is significant
2. Using lenient comparison for non-critical fields
3. Updating the expected metadata if the parser behavior has changed

### Issue: Test Case Not Found

If a test case is not found, consider:

1. Checking if the test case directory exists
2. Checking if the test case name is correct in the parameterized test list
3. Adding the test case if it's missing

## Best Practices

1. **Test Case Documentation**: Document what each test case is testing
2. **Regular Testing**: Run tests regularly during development
3. **Debug Output Analysis**: Analyze debug output for failures
4. **Consistent Comparison**: Use consistent comparison options
5. **Coverage Tracking**: Monitor test coverage

## Conclusion

The testing infrastructure for Python Readability is designed to ensure compatibility with the original Go implementation and reliability for end users. By following the guidelines in this document, you can effectively use and contribute to the testing infrastructure.